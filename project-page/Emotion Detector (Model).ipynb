{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "161ecc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D,MaxPooling2D\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95e98f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes=7\n",
    "\n",
    "# defines the size of the image array that we will be feeding to our neural network\n",
    "img_rows,img_cols=48,48\n",
    "\n",
    "# number of samples processed before the model is updated\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83c9798",
   "metadata": {},
   "source": [
    "# I. Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f34d227d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir='Z:\\MIT FutureMakers\\Week 4\\FER-2013/train'\n",
    "validation_data_dir='Z:\\MIT FutureMakers\\Week 4\\FER-2013/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45e464a",
   "metadata": {},
   "source": [
    "# II. Dataset Modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dd76c2",
   "metadata": {},
   "source": [
    "* **Image Augmentation**: a technique that can be used to artificially expand the size of a training dataset by creating modified versions of images in the dataset\n",
    "* Keras provides the capability to fit models using image data augmentation via the **ImageDataGenerator** class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65386267",
   "metadata": {},
   "source": [
    "**rotation_range**: Degree range for random rotations.<br />\n",
    "**shear_range**: Shear Intensity (Shear angle in counter-clockwise direction in degrees).<br /> \n",
    "**zoom_range**: Range for random zoom.<br />\n",
    "**width_shift_range**: This shifts the images by a value across its width.<br />\n",
    "**height_shift_range** : This shifts the images by a value across its height.<br />\n",
    "**horizontal_flip**: This flips the images horizontally.<br />\n",
    "**fill_mode**: This is used to fill in the pixels after making changes to the orientation of the images by the above used methods. Here, we're using ‘nearest’ as the fill mode to fill the missing pixels in the image using the nearby pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fb8d210",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    width_shift_range=0.4,\n",
    "    height_shift_range=0.4,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest')\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c584b37d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# rescaling the validation data\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    " train_data_dir,\n",
    " color_mode='grayscale',\n",
    " target_size=(img_rows,img_cols),\n",
    " batch_size=batch_size,\n",
    " class_mode='categorical',\n",
    " shuffle=True)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    " validation_data_dir,\n",
    " color_mode='grayscale',\n",
    " target_size=(img_rows,img_cols),\n",
    " batch_size=batch_size,\n",
    " class_mode='categorical',\n",
    " shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64158ae8",
   "metadata": {},
   "source": [
    "* In the above code, we used the **flow_from_directory()** method to load our dataset from the directory where the augmented images are stored \n",
    "* flow_from_directory() takes the path to a directory and generate batches of augmented data\n",
    "    * Here, we are giving some options to the method to automatically change the dimension and divide it in the classes so that it is easier to feed in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e42697",
   "metadata": {},
   "source": [
    "**directory**: The directory of the dataset<br />\n",
    "**color_mode**: Converting the images to gray-scale<br /> \n",
    "**target_size**: Convert the images to a uniform size (ie. 48x48)<br />\n",
    "**batch_size**: To make batches of data to train<br />\n",
    "**class_mode**: Here we are using ‘categorical’ as the class mode as we are categorizing the images into 7 classes.<br />\n",
    "**shuffle**: To shuffle the dataset for better training<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cffb8d",
   "metadata": {},
   "source": [
    "# III. Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496670f",
   "metadata": {},
   "source": [
    "We will be using a **Sequential model** which defines that all the layers in the network will be one after the other sequentially and storing it in a variable model.<br />\n",
    "* The model consists of 7 blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01db047e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23175c9",
   "metadata": {},
   "source": [
    "### Block 1\n",
    "\n",
    "* **Conv2D layer** - This layer creates a convolutional layer for the network. Here we are creating a layer with 32 filters and a filter size of (3,3) with padding=’same’ to pad the image and using the kernel initializer *he_normal*. We also have 2 convolutional layers, each followed by an activation and batch normalization layers.\n",
    "* **Activation layer** — elu activation.\n",
    "* **BatchNormalization** — Normalize the activations of the previous layer at each batch, i.e. applies a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "* **MaxPooling2D layer** — Downsamples the input representation by taking the maximum value over the window defined by pool_size for each dimension along the features axis. Here, the pool_size is (2,2).\n",
    "* **Dropout**: Dropout is a technique where randomly selected neurons are ignored during training. Here, since the dropout is 0.5, it will ignore half the neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68bb8d",
   "metadata": {},
   "source": [
    "### Block 2\n",
    "\n",
    "* Same layers as block-1 but the convolutional layers have 64 filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e860610",
   "metadata": {},
   "source": [
    "### Block 3\n",
    "\n",
    "* Same layers as block-1 but the convolutional layers have 128 filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffaaa7",
   "metadata": {},
   "source": [
    "### Block 4\n",
    "\n",
    "* Same layers as block-1 but the convolutional layers have 128 filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610aa58",
   "metadata": {},
   "source": [
    "### Block 5\n",
    "\n",
    "* **Flatten layer** — To flatten the output of the previous layers in a flat layer or in other words in the form of a vector (ie. a 1D layer)\n",
    "* **Dense layer** — A densely connected layer where each neuron is connected to every other neuron. Here, we are using 64 units or 64 neurons with a kernel initializer — he_normal.\n",
    "* These layers are followed by activation layer with elu activation , batch normalization and finally a dropout with 50% dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fdd0bb",
   "metadata": {},
   "source": [
    "### Block 6\n",
    "\n",
    "* Same layers as block 5 but without flatten layer as the input for this block is already flattened."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b78cff",
   "metadata": {},
   "source": [
    "### Block 7\n",
    "\n",
    "* **Dense layer** — In the final block of the network, we are using num_classes to create a dense layer having units=number of classes with a he_normal initializer.\n",
    "* **Activation layer** — Here, we are using a softmax layer which is typically used for multi-class classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d3fdfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Block-1\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n",
    "                 input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32,(3,3),padding='same',kernel_initializer='he_normal',\n",
    "                 input_shape=(img_rows,img_cols,1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Block-2\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Block-3\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Block-4\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),padding='same',kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#Block-5\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Block-6\n",
    "model.add(Dense(64,kernel_initializer='he_normal'))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#Block-7\n",
    "model.add(Dense(num_classes,kernel_initializer='he_normal'))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e583fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 455       \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 7)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,167\n",
      "Trainable params: 1,325,991\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the overall structure of the model\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f2ebff",
   "metadata": {},
   "source": [
    "# IV. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20bf6d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0282705",
   "metadata": {},
   "source": [
    "Before compiling, we will create 3 things using keras.callbacks class:\n",
    "\n",
    "1) **Checkpoint** (Function — *ModelCheckpoint()*) - monitors the validation loss and will try to minimize the loss using the mode=’min’ property. When the checkpoint is reached it will save the best trained weights.\n",
    "* **file-path**: Path to save the model file. Here, we are saving the model file with the name EmotionDetectionModel.h5\n",
    "* **monitor**: Quantity to monitor. Here, we are monitoring the validation loss.\n",
    "* **mode**: One of {auto, min, max}.\n",
    "* **save_best_only**: If save_best_only=True, the latest best model according to the quantity monitored will not be overwritten.\n",
    "* **verbose**: int. 0: quiet, 1: update messages.\n",
    "\n",
    "2) **Early Stopping** (Function — *EarlyStopping()*) - stops the execution early by checking the following properties.\n",
    "* **monitor**: Quantity to monitor. Here, we are monitoring the validation loss.\n",
    "* **min_delta**: Minimum change in the monitored quantity to qualify as an improvement, i.e. an absolute change of less than min_delta, will count as no improvement.\n",
    "* **patience**: Number of epochs with no improvement after which training will be stopped. \n",
    "* **restore_best_weights**: Whether to restore model weights from the epoch with the best value of the monitored quantity. If False, the model weights obtained at the last step of training are used.\n",
    "* **verbose**: int. 0: quiet, 1: update messages.\n",
    "\n",
    "3) **Reduce Learning Rate** (Function — *ReduceLROnPlateau()*) - Models often benefit from reducing the learning rate by a factor of 2–10 once learning stagnates. This callback monitors a quantity and if no improvement is seen for a ‘patience’ number of epochs, the learning rate is reduced\n",
    "* **monitor**: To monitor a particular loss. Here, we are monitoring the validation loss.\n",
    "* **factor**: Factor by which the learning rate will be reduced. new_lr = lr * factor. Here, we are using 0.2 as factor.\n",
    "* **patience**: Number of epochs with no improvement after which learning rate will be reduced.\n",
    "* **min_delta**: Threshold for measuring the new optimum, to only focus on significant changes.\n",
    "* **verbose**: int. 0: quiet, 1: update messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13f0ace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('EmotionDetectionModel.h5',\n",
    "                             monitor='val_loss',\n",
    "                             mode='min',\n",
    "                             save_best_only=True,\n",
    "                             verbose=1)\n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          min_delta=0,\n",
    "                          patience=3,\n",
    "                          verbose=1,\n",
    "                          restore_best_weights=True\n",
    "                          )\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=3,\n",
    "                              verbose=1,\n",
    "                              min_delta=0.0001)\n",
    "callbacks = [earlystop,checkpoint,reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34e9006",
   "metadata": {},
   "source": [
    "Now it’s time to finally compile the model using **model.compile()** and fit or train the model on the dataset using **model.fit_generator()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acdc777",
   "metadata": {},
   "source": [
    "**model.compile()** has the following parameters: \n",
    "* **loss**: This value will determine the type of loss function to use in your code. Here, we have categorical data in 5 categories or classes so we used ‘categorical_crossentropy’ loss.\n",
    "* **optimizer**: This value will determine the type of optimizer function to use in your code.Here, we used the Adam optimizer with learning rate 0.001 as it is the best optimizer for categorical data.\n",
    "* **metrics**: The metrics argument should be a list — your model can have any number of metrics. It is the list of metrics to be evaluated by the model during training and testing. Here we have used accuracy as the metric which will compile the model according to the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbee587",
   "metadata": {},
   "source": [
    "**model.fit_generator()** fits the model on data yielded batch-by-batch by a Python generator. It has the following parameters:\n",
    "* **generator**: The train_generator object that we created earlier.\n",
    "* **steps_per_epochs**: The steps to take on the training data in one epoch.\n",
    "* **epochs**: The total number of epochs (pass though the whole dataset once).\n",
    "* **callbacks**: The list containing all the callbacks that we created earlier.\n",
    "* **validation_data**: The validation_generator object that we created earlier.\n",
    "* **validation_steps**: The steps to take on the validation data in one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa506e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "897/897 [==============================] - 148s 162ms/step - loss: 2.1178 - accuracy: 0.1964 - val_loss: 1.7835 - val_accuracy: 0.2500\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.78354, saving model to EmotionDetectionModel.h5\n",
      "Epoch 2/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.8134 - accuracy: 0.2418 - val_loss: 1.7645 - val_accuracy: 0.2697\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.78354 to 1.76447, saving model to EmotionDetectionModel.h5\n",
      "Epoch 3/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.7886 - accuracy: 0.2560 - val_loss: 1.7503 - val_accuracy: 0.2730\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.76447 to 1.75025, saving model to EmotionDetectionModel.h5\n",
      "Epoch 4/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.7605 - accuracy: 0.2737 - val_loss: 1.6832 - val_accuracy: 0.3288\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.75025 to 1.68322, saving model to EmotionDetectionModel.h5\n",
      "Epoch 5/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.6910 - accuracy: 0.3148 - val_loss: 1.5482 - val_accuracy: 0.4134\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.68322 to 1.54824, saving model to EmotionDetectionModel.h5\n",
      "Epoch 6/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.6023 - accuracy: 0.3675 - val_loss: 1.3836 - val_accuracy: 0.4847\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.54824 to 1.38361, saving model to EmotionDetectionModel.h5\n",
      "Epoch 7/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.5310 - accuracy: 0.4078 - val_loss: 1.3407 - val_accuracy: 0.4807\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.38361 to 1.34065, saving model to EmotionDetectionModel.h5\n",
      "Epoch 8/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.4811 - accuracy: 0.4280 - val_loss: 1.3444 - val_accuracy: 0.4817\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.34065\n",
      "Epoch 9/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.4445 - accuracy: 0.4428 - val_loss: 1.2188 - val_accuracy: 0.5254\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.34065 to 1.21880, saving model to EmotionDetectionModel.h5\n",
      "Epoch 10/25\n",
      "897/897 [==============================] - 22s 25ms/step - loss: 1.4152 - accuracy: 0.4551 - val_loss: 1.2448 - val_accuracy: 0.5180\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.21880\n",
      "Epoch 11/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.3987 - accuracy: 0.4655 - val_loss: 1.1921 - val_accuracy: 0.5485\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.21880 to 1.19209, saving model to EmotionDetectionModel.h5\n",
      "Epoch 12/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.3819 - accuracy: 0.4737 - val_loss: 1.2218 - val_accuracy: 0.5378\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.19209\n",
      "Epoch 13/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.3595 - accuracy: 0.4790 - val_loss: 1.1596 - val_accuracy: 0.5522\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.19209 to 1.15960, saving model to EmotionDetectionModel.h5\n",
      "Epoch 14/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.3472 - accuracy: 0.4867 - val_loss: 1.1302 - val_accuracy: 0.5658\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.15960 to 1.13016, saving model to EmotionDetectionModel.h5\n",
      "Epoch 15/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.3381 - accuracy: 0.4958 - val_loss: 1.1515 - val_accuracy: 0.5601\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.13016\n",
      "Epoch 16/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.3274 - accuracy: 0.4963 - val_loss: 1.1193 - val_accuracy: 0.5742\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.13016 to 1.11928, saving model to EmotionDetectionModel.h5\n",
      "Epoch 17/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.3189 - accuracy: 0.5054 - val_loss: 1.0878 - val_accuracy: 0.5862\n",
      "\n",
      "Epoch 00017: val_loss improved from 1.11928 to 1.08779, saving model to EmotionDetectionModel.h5\n",
      "Epoch 18/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.3025 - accuracy: 0.5096 - val_loss: 1.0926 - val_accuracy: 0.5896\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.08779\n",
      "Epoch 19/25\n",
      "897/897 [==============================] - 23s 25ms/step - loss: 1.2977 - accuracy: 0.5166 - val_loss: 1.0908 - val_accuracy: 0.5864\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.08779\n",
      "Epoch 20/25\n",
      "897/897 [==============================] - 22s 25ms/step - loss: 1.2904 - accuracy: 0.5171 - val_loss: 1.0877 - val_accuracy: 0.5875\n",
      "\n",
      "Epoch 00020: val_loss improved from 1.08779 to 1.08767, saving model to EmotionDetectionModel.h5\n",
      "Epoch 21/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.2798 - accuracy: 0.5228 - val_loss: 1.1323 - val_accuracy: 0.5744\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.08767\n",
      "Epoch 22/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.2695 - accuracy: 0.5289 - val_loss: 1.0839 - val_accuracy: 0.5901\n",
      "\n",
      "Epoch 00022: val_loss improved from 1.08767 to 1.08388, saving model to EmotionDetectionModel.h5\n",
      "Epoch 23/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.2674 - accuracy: 0.5297 - val_loss: 1.0545 - val_accuracy: 0.6031\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.08388 to 1.05449, saving model to EmotionDetectionModel.h5\n",
      "Epoch 24/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.2592 - accuracy: 0.5322 - val_loss: 1.0540 - val_accuracy: 0.5992\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.05449 to 1.05398, saving model to EmotionDetectionModel.h5\n",
      "Epoch 25/25\n",
      "897/897 [==============================] - 23s 26ms/step - loss: 1.2530 - accuracy: 0.5352 - val_loss: 1.0481 - val_accuracy: 0.6064\n",
      "\n",
      "Epoch 00025: val_loss improved from 1.05398 to 1.04814, saving model to EmotionDetectionModel.h5\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    " optimizer = Adam(learning_rate=0.001),\n",
    " metrics=['accuracy'])\n",
    "nb_train_samples = 28709\n",
    "nb_validation_samples = 7178\n",
    "epochs=25\n",
    "\n",
    "history=model.fit(\n",
    " train_generator,\n",
    " steps_per_epoch=nb_train_samples//batch_size,\n",
    " epochs=epochs,\n",
    " callbacks=callbacks,\n",
    " validation_data=validation_generator,\n",
    " validation_steps=nb_validation_samples//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f42ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
